{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e60da921-d93a-49d3-9ef2-ea91da0ffad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phi.agent import Agent\n",
    "from phi.model.ollama import OllamaTools\n",
    "from phi.embedder.ollama import OllamaEmbedder\n",
    "from phi.knowledge.pdf import PDFUrlKnowledgeBase\n",
    "from phi.vectordb.lancedb import LanceDb, SearchType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee61d09-57a6-4d4c-b8cd-1b896567a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_HOST=\"localhost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc8e560-92ca-4d83-882f-f427de61fbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-11-16T20:49:47Z WARN  lance::dataset] No existing dataset at /Users/sroecker/code/AgenticAI-HandsOn/notebooks/tmp/lancedb/ocp.lance, it will be created\n"
     ]
    }
   ],
   "source": [
    "# Create a knowledge base from a PDF\n",
    "knowledge_base = PDFUrlKnowledgeBase(\n",
    "    urls=[\"https://docs.redhat.com/en-us/documentation/red_hat_openshift_ai_self-managed/2.15/pdf/serving_models/Red_Hat_OpenShift_AI_Self-Managed-2.15-Serving_models-en-US.pdf\"],\n",
    "    # Use LanceDB as the vector database\n",
    "    vector_db=LanceDb(\n",
    "        table_name=\"ocp\",\n",
    "        uri=\"tmp/lancedb\",\n",
    "        search_type=SearchType.vector,\n",
    "        # Change dimensions according to model context size and requirements\n",
    "        #embedder=OllamaEmbedder(model=\"mxbai-embed-large\", dimensions=512, host=OLLAMA_HOST), # doesn't work due to NaNs\n",
    "        embedder=OllamaEmbedder(model=\"nomic-embed-text\", dimensions=512, host=OLLAMA_HOST),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae893387-dfdc-483a-a175-30c32a16ca71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Creating collection                                                                                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m Creating collection                                                                                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading knowledge base                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m Loading knowledge base                                                                                    \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Reading:                                                                                                  \n",
       "         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://docs.redhat.com/en-us/documentation/red_hat_openshift_ai_self-managed/2.15/pdf/serving_models/Red_</span>\n",
       "         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">Hat_OpenShift_AI_Self-Managed-2.15-Serving_models-en-US.pdf</span>                                               \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m Reading:                                                                                                  \n",
       "         \u001b[4;94mhttps://docs.redhat.com/en-us/documentation/red_hat_openshift_ai_self-managed/2.15/pdf/serving_models/Red_\u001b[0m\n",
       "         \u001b[4;94mHat_OpenShift_AI_Self-Managed-2.15-Serving_models-en-US.pdf\u001b[0m                                               \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Added <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span> documents to knowledge base                                                                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mINFO    \u001b[0m Added \u001b[1;36m64\u001b[0m documents to knowledge base                                                                      \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Comment out after first run as the knowledge base is loaded\n",
    "knowledge_base.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37858f5b-3c01-41a7-b1aa-df6dd4923b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714b660d43374956b33408dad82a3c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    #model=OllamaTools(id=\"granite3-dense:2b-instruct-q8_0\"),\n",
    "    #model=OllamaTools(id=\"llama3.2:3b-instruct-q8_0\"),\n",
    "    #model=OllamaTools(id=\"hermes3:8b-llama3.1-q8_0\"),\n",
    "    model=OllamaTools(id=\"llama3.1:8b-instruct-q8_0\"),\n",
    "    markdown=True,\n",
    ")\n",
    "agent.print_response(\"How can LLMs be served on OpenShift AI?\", stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d6282c-c10e-4204-b0c7-cf837135dd19",
   "metadata": {},
   "source": [
    "That's ok but not perfect. Can we make it better? Let's include our knowledge base with the RHOAI model serving documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c46d357-3846-4f6c-9317-658959df9a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92797efb6b754ee8a4925f14f814a860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rag_agent = Agent(\n",
    "    #model=OllamaTools(id=\"granite3-dense:2b-instruct-q8_0\", options={\"num_ctx\": 4096}),\n",
    "    #model=OllamaTools(id=\"hermes3:8b-llama3.1-q8_0\", options={\"num_ctx\": 4096}),\n",
    "    # It's import to increase the context size from the Ollama default of 2048, otherwise the input will be truncated\n",
    "    model=OllamaTools(id=\"llama3.1:8b-instruct-q8_0\", options={\"num_ctx\": 4096}),\n",
    "    instructions=[\"Retrieve relevant OpenShift AI information from the knowledge base\"],\n",
    "    # Add the knowledge base to the agent\n",
    "    knowledge=knowledge_base,\n",
    "    add_references_to_prompt=True,\n",
    "    show_tool_calls=True,\n",
    "    markdown=True,\n",
    ")\n",
    "rag_agent.print_response(\"How can LLMs be served on OpenShift AI?\", stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba6abee-4e14-4619-b297-bebb86996c25",
   "metadata": {},
   "source": [
    "That's a much better answer for OpenShift AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
